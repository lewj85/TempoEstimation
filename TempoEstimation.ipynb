{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BiLSTM\n",
    "\n",
    "#from random import random\n",
    "#from numpy import array\n",
    "#from numpy import cumsum\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "#from keras.layers import Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "\n",
    "filename = 'asdf'\n",
    "\n",
    "inputs = prep_features(filename)\n",
    "tempos = blah\n",
    "\n",
    "\n",
    "# inputs and outputs\n",
    "nfft = \n",
    "nwin = \n",
    "a1 = Input(shape=(nwin,nfft/2 + 1))\n",
    "a2 = Input(shape=(nwin,nfft/2 + 1))\n",
    "a3 = Input(shape=(nwin,nfft/2 + 1))\n",
    "a4 = Input(shape=(nwin,nfft/2 + 1))\n",
    "a5 = Input(shape=(nwin,nfft/2 + 1))\n",
    "a6 = Input(shape=(nwin,nfft/2 + 1))\n",
    "alist = [a1,a2,a3,a4,a5,a6]\n",
    "a = keras.layers.Concatenate(axis=-1)(alist)\n",
    "\n",
    "def fork (model, n=2):\n",
    "    forks = []\n",
    "    for i in range(n):\n",
    "        f = Sequential()\n",
    "        f.add (model)\n",
    "        forks.append(f)\n",
    "    return forks\n",
    "\n",
    "# fork the inputs\n",
    "left, right = fork(a)\n",
    "\n",
    "# LSTM left branch\n",
    "left.add(LSTM(output_dim=hidden_units, init='uniform', inner_init='uniform',\n",
    "               forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "               inner_activation='sigmoid'))\n",
    "left.add(LSTM(output_dim=hidden_units, init='uniform', inner_init='uniform',\n",
    "               forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "               inner_activation='sigmoid'))\n",
    "left.add(LSTM(output_dim=hidden_units, init='uniform', inner_init='uniform',\n",
    "               forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "               inner_activation='sigmoid'))\n",
    "# LSTM right branch\n",
    "right.add(LSTM(output_dim=hidden_units, init='uniform', inner_init='uniform',\n",
    "               forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "               inner_activation='sigmoid', go_backwards=True))\n",
    "right.add(LSTM(output_dim=hidden_units, init='uniform', inner_init='uniform',\n",
    "               forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "               inner_activation='sigmoid', go_backwards=True))\n",
    "right.add(LSTM(output_dim=hidden_units, init='uniform', inner_init='uniform',\n",
    "               forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "               inner_activation='sigmoid', go_backwards=True))\n",
    "\n",
    "# combine\n",
    "output = Sequential()\n",
    "output.add(Merge([left, right], mode='concat'))\n",
    "# add dense at end - beat or no beat are the 2 units at end\n",
    "output.add(TimeDistributed(Dense(units=2, activation='softmax')))\n",
    "\n",
    "# stochastic gradient descent vs ADAM\n",
    "adm = Adam(lr=0.0001, decay=1e-5, momentum=0.9, nesterov=True)\n",
    "\n",
    "# create computational graph \n",
    "model = Model(inputs=alist, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adm)\n",
    "\n",
    "print(\"Train...\")\n",
    "model.fit(X_train, Y_train, batch_size=1, nb_epoch=nb_epoches, validation_data=(X_test, Y_test), verbose=1, show_accuracy=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_spectrogram_bilstm(nwin):\n",
    "    a1 = Input(shape=(nwin, 513))\n",
    "    a2 = Input(shape=(nwin, 1025))\n",
    "    a3 = Input(shape=(nwin, 2049)\n",
    "    a4 = Input(shape=(nwin, 513))\n",
    "    a5 = Input(shape=(nwin, 1025))\n",
    "    a6 = Input(shape=(nwin, 2049))\n",
    "    alist = [a1,a2,a3,a4,a5,a6]\n",
    "    a = keras.layers.Concatenate(axis=-1)(alist)\n",
    "    output = bilstm(a)\n",
    "    model = Model(inputs=alist, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-66ac4af68bed>, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-66ac4af68bed>\"\u001b[0;36m, line \u001b[0;32m32\u001b[0m\n\u001b[0;31m    return ms1.T, ms2., ms3, h1, h2, h3\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 6 spectrograms\n",
    "\n",
    "import librosa\n",
    "from scipy.signal import medfilt\n",
    "\n",
    "def prep_features(x, fs, hop_size=441):\n",
    "    \n",
    "    #x, fs = librosa.load(filename, sr=44100)\n",
    "\n",
    "    # 3 melspectrograms\n",
    "    s1 = librosa.core.stft(x, n_fft=1024, hop_length=441, win_length=1024, window='hamming', center=True, pad_mode='constant')\n",
    "    ms1 = librosa.feature.melspectrogram(sr=44100, S=s1)\n",
    "    s2 = librosa.core.stft(x, n_fft=2048, hop_length=441, win_length=2048, window='hamming', center=True, pad_mode='constant')\n",
    "    ms2 = librosa.feature.melspectrogram(sr=44100, S=s2)\n",
    "    s3 = librosa.core.stft(x, n_fft=4096, hop_length=441, win_length=4096, window='hamming', center=True, pad_mode='constant')\n",
    "    ms3 = librosa.feature.melspectrogram(sr=44100, S=s3)\n",
    "\n",
    "    # 3 median spectrograms\n",
    "    # pad edges\n",
    "    p1 = numpy.pad(ms1, ((0,0),(0,1024//2)), 'reflect')\n",
    "    p2 = numpy.pad(ms2, ((0,0),(0,2048//2)), 'reflect')\n",
    "    p3 = numpy.pad(ms3, ((0,0),(0,4096//2)), 'reflect')\n",
    "    # cut off the non-causal median leftover from the end\n",
    "    m1 = medfilt(p1, kernel_size=(1,1024//100))[:-1024/2]\n",
    "    m2 = medfilt(p2, kernel_size=(1,2048//100))[:-2048/2]\n",
    "    m3 = medfilt(p3, kernel_size=(1,4096//100))[:-4096/2]\n",
    "    # half-wave rectify\n",
    "    h1 = np.maximum(ms1-m1,0)\n",
    "    h2 = np.maximum(ms2-m2,0)\n",
    "    h3 = np.maximum(ms3-m3,0)\n",
    "\n",
    "    # transpose so time dimension is first\n",
    "    return ms1.T, ms2.T, ms3.T, h1.T, h2.T, h3.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN\n",
    "\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Flatten\n",
    "\n",
    "def cnn(x, num_steps, win_size=1024, weight_decay=0.01):\n",
    "\n",
    "    a = Input(shape=(num_steps, win_size))\n",
    "    filters = 256\n",
    "    kernel_size = 128\n",
    "    net = Conv1D(filters, kernel_size, strides=4, kernel_initializer='he_normal',\n",
    "               kernel_regularizer=keras.regularizers.l2(weight_decay))(a)\n",
    "    net = Conv1D(filters, kernel_size, strides=4, kernel_initializer='he_normal',\n",
    "               kernel_regularizer=keras.regularizers.l2(weight_decay))(net)\n",
    "    net = Conv1D(filters, kernel_size, strides=4, kernel_initializer='he_normal',\n",
    "               kernel_regularizer=keras.regularizers.l2(weight_decay))(net)\n",
    "    # remove channel dimension\n",
    "    net = Flatten(data_format='channels_last')(net)\n",
    "    \n",
    "    net = Dense(units=128, activation='linear')\n",
    "\n",
    "def fork (model, n=2):\n",
    "    \n",
    "    forks = []\n",
    "    for i in range(n):\n",
    "        f = Sequential()\n",
    "        f.add (model)\n",
    "        forks.append(f)\n",
    "        \n",
    "    return forks\n",
    "\n",
    "\n",
    "def bilstm(inputs):\n",
    "\n",
    "    # fork the inputs\n",
    "    left, right = fork(inputs)\n",
    "\n",
    "    # LSTM left branch\n",
    "    left.add(LSTM(output_dim=hidden_units, init='uniform', inner_init='uniform',\n",
    "                   forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "                   inner_activation='sigmoid'))\n",
    "    left.add(LSTM(output_dim=hidden_units, init='uniform', inner_init='uniform',\n",
    "                   forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "                   inner_activation='sigmoid'))\n",
    "    left.add(LSTM(output_dim=hidden_units, init='uniform', inner_init='uniform',\n",
    "                   forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "                   inner_activation='sigmoid'))\n",
    "    # LSTM right branch\n",
    "    right.add(LSTM(output_dim=hidden_units, init='uniform', inner_init='uniform',\n",
    "                   forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "                   inner_activation='sigmoid', go_backwards=True))\n",
    "    right.add(LSTM(output_dim=hidden_units, init='uniform', inner_init='uniform',\n",
    "                   forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "                   inner_activation='sigmoid', go_backwards=True))\n",
    "    right.add(LSTM(output_dim=hidden_units, init='uniform', inner_init='uniform',\n",
    "                   forget_bias_init='one', return_sequences=True, activation='tanh',\n",
    "                   inner_activation='sigmoid', go_backwards=True))\n",
    "\n",
    "    # combine\n",
    "    output = Sequential()\n",
    "    output.add(Merge([left, right], mode='concat'))\n",
    "    # add dense at end - beat or no beat are the 2 units at end\n",
    "    output.add(TimeDistributed(Dense(units=2, activation='softmax')))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.exists(\"/archive/r/rmb456/private_datasets/Beat_Tracking/hainsworth/info/001_info.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'artist': 'Dave Matthews Band',\n",
       " 'beats': array([  20064,   46050,   71272,   97640,  124678,  152018,  178895,\n",
       "         204642,  231774,  258143,  284129,  310879,  338012,  365144,\n",
       "         390748,  416352,  443600,  468821,  497669,  524233,  550219,\n",
       "         576205,  603338,  629324,  656839,  683207,  709957,  736326,\n",
       "         763076,  789445,  816195,  842563,  868282,  895032,  921401,\n",
       "         948151,  973373, 1000505, 1025345, 1051713, 1078846, 1106361,\n",
       "        1132347, 1158715, 1185848, 1212565, 1238708, 1264363, 1291428,\n",
       "        1317804, 1343790, 1370922, 1396909, 1422513, 1448881, 1474867,\n",
       "        1501617, 1529132, 1555118, 1582633, 1608619, 1634987, 1660209,\n",
       "        1687552, 1714971, 1740985, 1766769, 1793312, 1820827, 1847577,\n",
       "        1872799, 1899549, 1926682, 1953050, 1979418, 2006169, 2032919,\n",
       "        2060434, 2085656, 2111260, 2139017, 2166863, 2193785, 2219367,\n",
       "        2246450, 2272848, 2298559, 2323586, 2349983, 2376380, 2402092,\n",
       "        2429518, 2456258, 2483805], dtype=int32),\n",
       " 'difficulty': 2,\n",
       " 'duration': 56.47095238095238,\n",
       " 'filename': '1',\n",
       " 'style': 'dance',\n",
       " 'subbeat_structure': 'semiquavers',\n",
       " 'tempo': 99.9563288322695,\n",
       " 'tempo_oddities': 'normal',\n",
       " 'title': \"Best of What's Around\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "HAINSWORTH_STYLES = [\n",
    "    'classical',\n",
    "    'choral',\n",
    "    'rock/pop',\n",
    "    'dance',\n",
    "    'unused',\n",
    "    'jazz',\n",
    "    'big band jazz',\n",
    "    '60s pop',\n",
    "    'folk',\n",
    "    'random stuff'\n",
    "]\n",
    "\n",
    "HAINSWORTH_TEMPO_ODDITIES = [\n",
    "    'normal',\n",
    "    'rall',\n",
    "    'sudden change',\n",
    "    'rubato'\n",
    "]\n",
    "\n",
    "HAINSWORTH_SUBBEAT_STRUCTURES = [\n",
    "    'not divided',\n",
    "    'quavers',\n",
    "    'semiquavers',\n",
    "    'demisemiquavers',\n",
    "    'triplets',\n",
    "    'into 6',\n",
    "    'swing',\n",
    "    'swung semiquavers'\n",
    "]\n",
    "\n",
    "def load_hainsworth_annotations(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        raise ValueError('{} does not exist'.format(filename))\n",
    "    \n",
    "    datarec = loadmat(filename)['datarec']\n",
    "    filename = datarec[0][0][0]\n",
    "    artist = datarec[1][0][0]\n",
    "    title = datarec[2][0][0]\n",
    "    duration = float(datarec[3][0][0][0])\n",
    "    style_num = int(datarec[4][0][0][0])\n",
    "    tempo_oddity_num = int(datarec[5][0][0][0])\n",
    "    difficulty_num = int(datarec[6][0][0][0])\n",
    "    subbeat_struct_num = int(datarec[7][0][0][0])\n",
    "    tempo = float(datarec[8][0][0][0])\n",
    "    num_beats = int(datarec[9][0][0][0])\n",
    "    beats = datarec[10][0].flatten()\n",
    "    \n",
    "    assert len(beats) == num_beats\n",
    "    \n",
    "\n",
    "    y = beats[0].flatten()\n",
    "    annotations = {\n",
    "        'filename': filename,\n",
    "        'artist': artist,\n",
    "        'title': title,\n",
    "        'duration': duration,\n",
    "        'style': HAINSWORTH_STYLES[style_num - 1],\n",
    "        'tempo_oddities': HAINSWORTH_TEMPO_ODDITIES[tempo_oddity_num - 1],\n",
    "        'difficulty': difficulty_num,\n",
    "        'subbeat_structure': HAINSWORTH_SUBBEAT_STRUCTURES[subbeat_struct_num - 1],\n",
    "        'tempo': tempo,\n",
    "        'beats': beats\n",
    "    }\n",
    "    return annotations\n",
    "\n",
    "def resample_hainsworth_beats(annotations, source_sr, target_sr):\n",
    "    annotations['beats'] = ((target_sr/source_sr) * annotations['beats']).astype(int)\n",
    "\n",
    "load_hainsworth_annotations(\"/beegfs/jtc440/aca/Beat_Tracking/hainsworth/info/001_info.mat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/beegfs/jtc440/miniconda3/envs/l3embedding-cpu/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/beegfs/jtc440/miniconda3/envs/l3embedding-cpu/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import resampy\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "def prep_hainsworth_data(info_dir, sample_dir, target_sr=44100):\n",
    "    audio_array = []\n",
    "    label_array = []\n",
    "\n",
    "    #infodir = \"./info/\"\n",
    "    #sampledir = \"./samples/\"\n",
    "\n",
    "    for i in range(245):\n",
    "        # {} for replacement, : format spec, 03 pad up to 3 leading 0s, d is int\n",
    "        filename1 = \"{:03d}_info.mat\".format(i)\n",
    "        filename2 = \"{:03d}.wav\".format(i)\n",
    "\n",
    "        f1 = os.path.join(info_dir, filename1)\n",
    "        f2 = os.path.join(sample_dir, filename2)\n",
    "        if os.path.exists(f1) and os.path.exists(f2):\n",
    "            # Load audio at original sample rate\n",
    "            x, sr = librosa.load(f2, sr=None)\n",
    "            r = load_hainsworth_annotations(f1)\n",
    "            resample_hainsworth_beats(r, sr, target_sr)\n",
    "            \n",
    "            # Resample audio to target sample rate\n",
    "            x = resampy.resample(x, fs, target_sr)\n",
    "            \n",
    "            audio_array.append(x)\n",
    "            label_array.append(r) # has tempos too\n",
    "\n",
    "    return audio_array, label_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(audio_array, label_array, hop_size=441, mode='spectrogram', sr=44100):\n",
    "\n",
    "    if mode == 'spectrogram':\n",
    "        X = (sequence(feat) for feat in zip(*[prep_features(x, sr) for x in audio_array]))\n",
    "    elif mode == 'audio':\n",
    "        X = sequence(audio_array)\n",
    "        \n",
    "    y = sequence([keras.utils.to_categorical((labels / hop_size).astype(int), num_classes=len(labels))\n",
    "                  for labels in label_array])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info_dir = \"./info/\"\n",
    "sample_dir = \"./samples/\"\n",
    "hop_size = 1234\n",
    "target_fs = 1234\n",
    "batch_size = 5\n",
    "num_epochs = 10\n",
    "\n",
    "# prepare the data\n",
    "a, r = prep_hainsworth_data(info_dir, sample_dir, hop_size, target_fs)\n",
    "#a, r = prep_ballroom_data(info_dir, sample_dir, hop_size, target_fs)\n",
    "\n",
    "# 6 spectrograms\n",
    "X, y = preprocess_data(a, r, mode='spectrogram')\n",
    "\n",
    "# create BiLSTM and dense\n",
    "model = construct_spectrogram_bilstm(X[0].shape[0])\n",
    "\n",
    "# create computational graph \n",
    "adm = Adam(lr=0.0001, decay=1e-5, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adm)\n",
    "# training\n",
    "model.fit(x=X, y=y, epochs=num_epochs, batch_size=batch_size)\n",
    "\n",
    "# testing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
